{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tensor Network Tutorial",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/K-MkrOps/4_kghcr1_2/blob/master/Tensor_Network_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F41jzPsm_9tZ",
        "outputId": "4f07efed-55e9-4454-eea5-ab609cd6f978",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install tensornetwork jax jaxlib\n",
        "import numpy as np\n",
        "import jax\n",
        "import tensornetwork as tn"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensornetwork in /usr/local/lib/python3.7/dist-packages (0.4.6)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.7/dist-packages (0.3.8)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.7/dist-packages (0.3.7+cuda11.cudnn805)\n",
            "Requirement already satisfied: opt-einsum>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from tensornetwork) (3.3.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensornetwork) (3.1.0)\n",
            "Requirement already satisfied: graphviz>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from tensornetwork) (0.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from tensornetwork) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.1 in /usr/local/lib/python3.7/dist-packages (from tensornetwork) (1.4.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensornetwork) (1.5.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from jax) (1.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jax) (4.1.1)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from jaxlib) (2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaWIxu_4aBNj"
      },
      "source": [
        "# Section 1: Basic usage.\n",
        "In this section, we will go over basic linear algebra operations and how to create them using a TensorNetwork. While at first it may seem more complicated to use a tensornetwork rather than just doing the operations by hand, we will use the skills developed in this section to start building and contracting very complicated tensor networks that would be very difficult to do otherwise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4i5M1zLbbsL"
      },
      "source": [
        "Let's begin by doing the most basic operation possible, a vector dot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "outputId": "552199d8-2e39-4fce-edb0-a0e7a2dbcdba",
        "id": "R_9ZTrSoAah3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Next, we add the nodes containing our vectors.\n",
        "a = tn.Node(np.ones(10))\n",
        "# Either tensorflow tensors or numpy arrays are fine.\n",
        "b = tn.Node(np.ones(10))\n",
        "# We \"connect\" these two nodes by their \"0th\" edges.\n",
        "# This line is equal to doing `tn.connect(a[0], b[0])\n",
        "# but doing it this way is much shorter.\n",
        "edge = a[0] ^ b[0]\n",
        "# Finally, we contract the edge, giving us our new node with a tensor\n",
        "# equal to the inner product of the two earlier vectors\n",
        "c = tn.contract(edge)\n",
        "# You can access the underlying tensor of the node via `node.tensor`.\n",
        "# To convert a Eager mode tensorflow tensor into \n",
        "print(c.tensor)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gf_tCdk7lNeN"
      },
      "source": [
        "## Edge-centric connection.\n",
        "When a node is created in the TensorNetwork, that node is automatically filled with dangling-edges. To connect two nodes together, we actually remove the two danging edges in the nodes and replace them with a standard/trace edge."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLuYsq_clLTA",
        "outputId": "a7e4128c-a2e7-4ada-fc16-719210554615",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "a = tn.Node(np.eye(2))\n",
        "# Notice that a[0] is actually an \"Edge\" type.\n",
        "print(\"The type of a[0] is:\", type(a[0]))\n",
        "# This is a dangling edge, so this method will \n",
        "print(\"Is a[0] dangling?:\", a[0].is_dangling())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The type of a[0] is: <class 'tensornetwork.network_components.Edge'>\n",
            "Is a[0] dangling?: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySDV2F3lm5_S"
      },
      "source": [
        "Now, let's connect a[0] to a[1]. This will create a \"trace\" edge."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1BBz3Zymuea",
        "outputId": "2dcca5a3-e215-4c85-d5cb-e894caf1b39a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "trace_edge = a[0] ^ a[1]\n",
        "# Notice now that a[0] and a[1] are actually the same edge.\n",
        "print(\"Are a[0] and a[1] the same edge?:\", a[0] is a[1])\n",
        "print(\"Is a[0] dangling?:\", a[0].is_dangling())"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Are a[0] and a[1] the same edge?: True\n",
            "Is a[0] dangling?: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cW-IWA_l0By"
      },
      "source": [
        "## Axis naming.\n",
        "Sometimes, using the axis number is very inconvient and it can be hard to keep track of the purpose of certain edges. To make it easier, you can optionally add a name to each of the axes of your node. Then you can get the respective edge by indexing using the name instead of the number."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "outputId": "3606cfbf-ef06-4580-e0c2-b86dbcb6d3a9",
        "id": "M0k91fG9A-nc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Here, a[0] is a['alpha'] and a[1] is a['beta']\n",
        "a = tn.Node(np.eye(2), axis_names=['alpha', 'beta'])\n",
        "edge = a['alpha'] ^ a['beta']\n",
        "result = tn.contract(edge)\n",
        "print(result.tensor)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srvhkdXyq93b"
      },
      "source": [
        "# Section 2. Advanced Network Contractions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgEhqwUV0tmd"
      },
      "source": [
        "## Avoid trace edges.\n",
        "While the TensorNetwork library fully supports trace edges, contraction time is ALWAYS faster if you avoid creating them. This is because trace edges only sum the diagonal of the underlying matrix, and the rest of the values (which is a majorit of the total values) are just garbage. You both waste compute time and memory by having these useless trace edges.\n",
        "\n",
        "The main way we support avoid trace edges is via the `@` symbol, which is an alias to `tn.contract_between`. Take a look at the speedups!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqlIH0wOqvd0",
        "outputId": "12e1371a-e5d8-4970-f6aa-46777094e972",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def one_edge_at_a_time(a, b):\n",
        "  node1 = tn.Node(a)\n",
        "  node2 = tn.Node(b)\n",
        "  edge1 = node1[0] ^ node2[0]\n",
        "  edge2 = node1[1] ^ node2[1]\n",
        "  tn.contract(edge1)\n",
        "  result = tn.contract(edge2)\n",
        "  return result.tensor\n",
        "\n",
        "def use_contract_between(a, b):\n",
        "  node1 = tn.Node(a)\n",
        "  node2 = tn.Node(b)\n",
        "  node1[0] ^ node2[0]\n",
        "  node1[1] ^ node2[1]\n",
        "  # This is the same as \n",
        "  # tn.contract_between(node1, node2)\n",
        "  result = node1 @ node2\n",
        "  return result.tensor\n",
        "\n",
        "a = np.ones((1000, 1000))\n",
        "b = np.ones((1000, 1000))\n",
        "print(\"Running one_edge_at_a_time\")\n",
        "%timeit one_edge_at_a_time(a, b)\n",
        "print(\"Running use_cotract_between\")\n",
        "%timeit use_contract_between(a, b)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running one_edge_at_a_time\n",
            "10 loops, best of 5: 87.4 ms per loop\n",
            "Running use_cotract_between\n",
            "100 loops, best of 5: 2.48 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFgUTwnt3Umg"
      },
      "source": [
        "We also have `contract_parallel` which does the same thing as `contract_between`, only you pass a single edge instead of two nodes. This will contract all of the edges \"parallel\" to the given edge (meaning all of the edges that share the same two nodes as the given edge).\n",
        "\n",
        "Using either method is fine and they will do the exact same thing. In fact, if you look at the source code, `contract_parallel` actually just calls `contract_between`. :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYXoLPNA22fm",
        "outputId": "30583a58-3153-420e-fd4d-982264a22834",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def use_contract_parallel(a, b):\n",
        "  node1 = tn.Node(a)\n",
        "  node2 = tn.Node(b)\n",
        "  edge = node1[0] ^ node2[0]\n",
        "  node1[1] ^ node2[1]\n",
        "  result = tn.contract_parallel(edge)\n",
        "  # You can use `get_final_node` to make sure your network \n",
        "  # is fully contracted.\n",
        "  return result.tensor\n",
        "\n",
        "print(\"Running contract_parallel\")\n",
        "%timeit use_contract_parallel(a, b)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running contract_parallel\n",
            "1000 loops, best of 5: 938 µs per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyUb_CyaZqmv"
      },
      "source": [
        "## Complex Contraction.\n",
        "Remember this crazy hard to write tensor contraction?\n",
        "Well, we're gonna do it in about 13 lines of simple code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzAlIQ9eyv8Z",
        "outputId": "12b95e11-7a82-42d2-a81a-65d31af32889",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Here, we will contract the following shaped network.\n",
        "# O - O\n",
        "# | X |\n",
        "# O - O\n",
        "a = tn.Node(np.ones((2, 2, 2)))\n",
        "b = tn.Node(np.ones((2, 2, 2)))\n",
        "c = tn.Node(np.ones((2, 2, 2)))\n",
        "d = tn.Node(np.ones((2, 2, 2)))\n",
        "# Make the network fully connected.\n",
        "a[0] ^ b[0]\n",
        "a[1] ^ c[1]\n",
        "a[2] ^ d[2]\n",
        "b[1] ^ d[1]\n",
        "b[2] ^ c[2]\n",
        "c[0] ^ d[0]\n",
        "# We are using the \"greedy\" contraction algorithm.\n",
        "# Other algorithms we support include \"optimal\" and \"branch\".\n",
        "\n",
        "# Finding the optimial contraction order in the general case is NP-Hard,\n",
        "# so there is no single algorithm that will work for every tensor network.\n",
        "# However, there are certain kinds of networks that have nice properties that\n",
        "# we can expliot to making finding a good contraction order easier.\n",
        "# These types of contraction algorithms are in developement, and we welcome \n",
        "# PRs!\n",
        "\n",
        "# `tn.reachable` will do a BFS to get all of the nodes reachable from a given\n",
        "# node or set of nodes.\n",
        "# nodes = {a, b, c, d}\n",
        "nodes = tn.reachable(a)\n",
        "result = tn.contractors.greedy(nodes)\n",
        "print(result.tensor)\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xZybTdRVaeq",
        "outputId": "81c50b56-39a4-4ffc-81c9-238dfd77aac9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# To make connecting a network a little less verbose, we have included\n",
        "# the NCon API aswell.\n",
        "\n",
        "# This example is the same as above.\n",
        "ones = np.ones((2, 2, 2))\n",
        "tn.ncon([ones, ones, ones, ones], \n",
        "        [[1, 2, 4], \n",
        "         [1, 3, 5], \n",
        "         [2, 3, 6],\n",
        "         [4, 5, 6]])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(64.)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2x1GbZcYWTrT",
        "outputId": "79d026ab-a443-4e63-be85-f477bb94819e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# To specify dangling edges, simply use a negative number on that index.\n",
        "\n",
        "ones = np.ones((2, 2))\n",
        "tn.ncon([ones, ones], [[-1, 1], [1, -2]])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2., 2.],\n",
              "       [2., 2.]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SQz6EzVuOnk"
      },
      "source": [
        "# Section 3: Node splitting.\n",
        "In the final part of this colab, will go over the SVD node splitting methods."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVOluV8-4YPD"
      },
      "source": [
        "# To make the singular values very apparent, we will just take the SVD of a\n",
        "# diagonal matrix.\n",
        "diagonal_array = np.array([[2.0, 0.0, 0.0],\n",
        "                           [0.0, 2.5, 0.0],\n",
        "                           [0.0, 0.0, 1.5]]) "
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUc-1xWnwcDY",
        "outputId": "c9fbf0f1-905b-46ff-a77a-92718e399341",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# First, we will go over the simple split_node method.\n",
        "a = tn.Node(diagonal_array)\n",
        "u, vh, _ = tn.split_node(\n",
        "    a, left_edges=[a[0]], right_edges=[a[1]])\n",
        "print(\"U node\")\n",
        "print(u.tensor)\n",
        "print()\n",
        "print(\"V* node\")\n",
        "print(vh.tensor)\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "U node\n",
            "[[0.         1.41421356 0.        ]\n",
            " [1.58113883 0.         0.        ]\n",
            " [0.         0.         1.22474487]]\n",
            "\n",
            "V* node\n",
            "[[0.         1.58113883 0.        ]\n",
            " [1.41421356 0.         0.        ]\n",
            " [0.         0.         1.22474487]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGlRgZq82LP2",
        "outputId": "44ab33a3-f6f8-43b7-ffca-7032b4dc1796",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Now, we can contract u and vh to get back our original tensor!\n",
        "print(\"Contraction of U and V*:\")\n",
        "print((u @ vh).tensor)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contraction of U and V*:\n",
            "[[2.  0.  0. ]\n",
            " [0.  2.5 0. ]\n",
            " [0.  0.  1.5]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkBXbOC65qnj"
      },
      "source": [
        "We can also drop the lowest singular values in 2 ways, \n",
        "1. By setting `max_singular_values`. This is the maximum number of the original\n",
        "singular values that we want to keep.\n",
        "2. By setting `max_trun_error`. This is the maximum amount the sum of the removed singular values can be."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQZQn4T32YJ3",
        "outputId": "fa130793-cbaa-4b3c-ec48-2f3d6a77dc33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# We can also drop the lowest singular values in 2 ways, \n",
        "# 1. By setting max_singular_values. This is the maximum number of the original\n",
        "# singular values that we want to keep.\n",
        "a = tn.Node(diagonal_array)\n",
        "u, vh, truncation_error = tn.split_node(\n",
        "    a, left_edges=[a[0]], right_edges=[a[1]], max_singular_values=2)\n",
        "# Notice how the two largest singular values (2.0 and 2.5) remain\n",
        "# but the smallest singular value (1.5) is removed.\n",
        "print((u @ vh).tensor)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2.  0.  0. ]\n",
            " [0.  2.5 0. ]\n",
            " [0.  0.  0. ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYXIXUYY3nf7"
      },
      "source": [
        "We can see the values of the removed singular values by looking at the returned `truncation_error`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXL9HSmJ3OWe",
        "outputId": "14173cad-10f5-4c20-b503-e229ef2a3583",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# truncation_error is just a normal tensorflow tensor.\n",
        "print(truncation_error)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycqQuPYSEnP2"
      },
      "source": [
        "# Section 4: running on GPUs\n",
        "\n",
        "To get this running on a GPU, we recommend using the JAX backend, as it has nearly the exact same API as numpy.\n",
        "\n",
        "To get a GPU, go to Runtime -> Change runtime type -> GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQKueu8aEqwM",
        "outputId": "f2a5cd3f-a7b7-4d12-e703-335636b6457a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def calculate_abc_trace(a, b, c):\n",
        "  an = tn.Node(a)\n",
        "  bn = tn.Node(b)\n",
        "  cn = tn.Node(c)\n",
        "  an[1] ^ bn[0]\n",
        "  bn[1] ^ cn[0]\n",
        "  cn[1] ^ an[0]\n",
        "  return (an @ bn @ cn).tensor\n",
        "\n",
        "a = np.ones((4096, 4096))\n",
        "b = np.ones((4096, 4096))\n",
        "c = np.ones((4096, 4096))\n",
        "\n",
        "tn.set_default_backend(\"numpy\")\n",
        "print(\"Numpy Backend\")\n",
        "%timeit calculate_abc_trace(a, b, c)\n",
        "tn.set_default_backend(\"jax\")\n",
        "# Running with a GPU: 202 ms\n",
        "# Running with a CPU: 2960 ms\n",
        "print(\"JAX Backend\")\n",
        "%timeit np.array(calculate_abc_trace(a, b, c))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numpy Backend\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 5: 3.96 s per loop\n",
            "JAX Backend\n",
            "1 loop, best of 5: 2.56 s per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaJpprAMWI7S"
      },
      "source": [
        ""
      ],
      "execution_count": 27,
      "outputs": []
    }
  ]
}